##  MAT3007: Optimization - Assignment 7

Ran Hu  116010078 

21 November, 2018

### Problem 1

1. False.

   Let $f(x) = x^2 $ ; $g(x) = -lnx,\ x > 0$ , so both $f(x)$ and $g(x)$ are convex.

   Get the second derivative: 
   $$
   f(g(x)) = (lnx)^2 \\
   f'(g(x)) = \frac{2}{x}lnx \\
   f''(g(x)) = \frac{2}{x^2}(1-lnx)
   $$
   (i) when $1-lnx \geq 0$ , i.e. $0 < x \leq e$ , $f''(g(x)) \geq 0$ , so $f(g(x))$ is convex;

   (ii) when $1-lnx \leq 0$, i.e. $x\geq e$ , $f''(g(x)) \leq 0$ , so $f(g(x))$ is concave. 

   Therefore, if $f(x)$ is convex, $g(x)$ is convex, $f(g(x))$ is not necessarily convex. 

2. True.

   Because $g(x)$ is convex, for every $x_1,\ x_2\in Ω$ ($g(x)$ is on $Ω$) and any $0 \leq \alpha \leq 1$ ,
   $$
   g(αx_1 +(1−α)x_2) ≤ αg(x_1)+(1−α)g(x_2) \\
   $$
   Because $f(x)$ is nondecreasing and convex, for every $αg(x_1),\ (1−α)g(x_2) \in Ω'$ ($f(x)$ is on $Ω'$) we have
   $$
   f(g(αx_1 +(1−α)x_2)) ≤ f(αg(x_1)+(1−α)g(x_2)) \leq \alpha f(g(x_1)) + (1-\alpha)f(g(x_2))
   $$
   Therefore, if $f(x)$ is convex and nondecreasing, $g(x)$ is convex, as long as $g(x) \in \Omega'$ , then $f(g(x))$ is convex.​      

3. False.

   Because $g(x)$ is convex, for every $x_1,\ x_2\in Ω$ ($g(x)$ is on $Ω$) and any $0 \leq \alpha \leq 1$ ,
   $$
   g(αx_1 +(1−α)x_2) ≤ αg(x_1)+(1−α)g(x_2) \\
   $$
   Because $f(x)$ is nonincreasing and concave, for every $αg(x_1),\ (1−α)g(x_2) \in Ω'$ ($f(x)$ is on $Ω'$) we have
   $$
   f(g(αx_1 +(1−α)x_2)) \geq f(αg(x_1)+(1−α)g(x_2)) \geq \alpha f(g(x_1)) + (1-\alpha)f(g(x_2))
   $$
   Therefore, if $f(x)$ is convex and nondecreasing, $g(x)$ is convex, as long as $g(x) \in \Omega'$ , then $f(g(x))$ is concave.  

4. False.

   Since $f(x)$ is increasing and non-negative, $f'(x) > 0, f(x) \geq 0, x \geq 0$ .

   (a) If $f(x) $ is continuously differentiable on $x \geq 0$ ,

   (i) when $x > 0$ ,
   $$
   (xf(x))' = f(x)+xf'(x) > 0 \Rightarrow f'(x) > -\frac{f(x)}{x} \\
   (xf(x))'' = 2f'(x) + xf''(x) > -\frac{2f(x)}{x} + xf''(x)
   $$
   if $(xf(x))'' \geq 0$ , $f''(x) \geq \frac{-2f'(x)}{x}$ ; if $f''(x) \geq \frac{2f(x)}{x^2}$ , $(xf(x))'' > 0$ . 

   (ii) when $x = 0,\ (xf(x))'' = 2f'(x) > 0$ 

   Therefore, $xf(x)$ is convex on $x \geq 0$ only if $ f''(x) \geq \frac{-2f'(x)}{x}$ . If $ f''(x) \leq \frac{-2f'(x)}{x}$, then $xf(x)$ is concave.  

   (b) If $f(x)$ are segmented functions on $x \geq 0$ , a counter example can be $ f(x) = x^2, 0\leq x < 1;\ f(x)=x,\ x \geq 1$ . 

   In this case, $xf(x)$ is not convex, since it does not satisfy $f(αx_1 +(1−α)x_2)≤αf(x_1)+(1−α)f(x_2)$ when $x_1 \in [0,\ 1],\ x_2 \in [1, \infty)$ .  



### Problem 2

1. For every $x_1',\ x_1'',\ x_2',\ x_2'' \in R$ and any $0 ≤ α ≤ 1 $ , 

$$
f(\alpha x_1'+(1-\alpha)x_1'',\ \alpha x_2'+(1-\alpha)x_2'') 
= log(e^{\alpha x_1'+(1-\alpha)x_1''}+e^{\alpha x_2'+(1-\alpha)x_2''}) \\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \
=log(e^{\alpha x_1'}e^{(1-\alpha)x_1''}+e^{\alpha x_2'}e^{(1-\alpha)x_2''}) \\
\qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad 
\leq log((e^{x_1'}+e^{x_2'})^\alpha(e^{x_1''}+e^{x_2''})^{(1-\alpha)} )\\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
= \alpha log(e^{x_1'}+e^{x_2'}) + (1-\alpha)log(e^{x_1''}+e^{x_2''})\\
\qquad \qquad \qquad \qquad \qquad \qquad  \quad \quad \ \
=\alpha f(x_1',\ x_2') + (1-\alpha)f(x_1'',\ x_2'')
$$

Because $f(\alpha x_1'+(1-\alpha)x_1'',\ \alpha x_2'+(1-\alpha)x_2'')  \leq \alpha f(x_1',\ x_2') + (1-\alpha)f(x_1'',\ x_2'')$ , $f$ is a convex function in $(x1,\ x2)$ .

2. let $x = e^a,\ y = e^b,\ z = e^c$ . The convex optimization problem is:
   $$
   \rm{minimize} \qquad \qquad e^{2c} \\
   s.t.  
   \ \quad \qquad  a \geq -10 \\
   \qquad \qquad a \leq 3 \\
    \qquad \qquad log(e^{2a-\frac{b}{2}}+e^{b-\frac{a}{2}}) \leq 0 \\
   \qquad \qquad a - b - 2c = 0
   $$



3. 

3. ```matlab
   cvx_begin quiet
       variables a b c;
       minimize exp(2*c)
       subject to
           3 >= a >= -10
           a-b-2*c == 0
           log(exp(2*a)+exp(b-c)) <= b/2
   cvx_end
   
   [a b c]
   cvx_opt = cvx_optval
   ```

   The results are: $a = -10,\ b = -5,\ c = -2.5$ , which means $x = e^{-10},\ y = e^{-5},\ z=e^{-2.5}$ . The optimal solution is 0.00673795 .   



### Problem 3

Its Hessian matrix is:

![1](/Users/rose/Desktop/A7/1.jpeg)

The eigenvalues are 0.8299, 2.6889, and 4.4812, which shows that the Hessian matrix is positive semi-definite (PSD) throughout the defined region. So $f(x,y,z)=2x^2 +xy+y^2 +yz+z^2 −6x−7y−8z−9$ is convex.  

```matlab
cvx_begin quiet
    variables x y z
    minimize ((x+1/2*y)^2 + (y/2+z)^2 + (x-3)^2 + 1/2*(y-7)^2 - 8*z - 85/2)
cvx_end

[x y z]
cvx_opt = cvx_optval
```

The optimal solution is $x =  1.2,\ y = 1.2,\ z = 3.4$ . The optimal value is $-30.4 ​$ . 



### Problem 4

The optimization problem is:
$$
\rm{minimize}\qquad \qquad \sum^n_{i=1}x_ilogx_i\\
s.t. \qquad \qquad \sum^n_{i=1}a_ix_i = 1\\
\ \qquad \qquad \qquad \qquad x_i \geq 0
$$
Let $f(x) = xlogx$ , then $f'(x) = logx + 1, f''(x) = \frac{1}{x}$ . When $x \geq 0$ , $f''(x) > 0$ , so $f(x)$ is convex. 

Since $f(x_i) = x_ilogx_i,\ \forall i \in [1,\ n]$ are convex functions, $\sum^n_{i=1}f(x_i)$ is a convex function. 

Additionally, $\sum^n_{i=1}a_ix_i = 1$ is a linear constraint, so it is a convex constraint. $g(x)=x$ is concave, so $x_i \geq 0$ are all convex constraints. 

Therefore,  the constraints is a convex feasible region. 

In conclusion, it is a convex optimization problem, because it minimizes a convex function over a convex feasible region.



### Problem 5

1. ![](/Users/rose/Desktop/A7/3.jpeg)

The picture shows that $r(p) = pλ(p)$ is not concave.

2. 

2. $$
   p = log{\frac{1-λ}{λ}}\\
   \widetilde r(λ) = λlog{\frac{1-λ}{λ}} \\
   \widetilde r'(λ) = log{\frac{1-λ}{λ}} - \frac{1}{1-λ} \\
   \widetilde r''(λ) = -\frac{1}{(1-λ)^2\lambda}
   $$

   Since $p \geq 0,\ λ(p) = \frac{e^{−p}}{1+e^{−p}} \in (0,\ \frac{1}{2})$ , $\widetilde r''(λ)  < 0$ . Therefore, $\widetilde r''(λ)$  is concave in λ.

4. The optimization problem is:
   $$
   \rm{maximize}_\lambda \qquad λlog{\frac{1-λ}{λ}} \\
   s.t. \qquad \qquad \quad \lambda - \frac{1}{2} \leq 0 \\
   \qquad \qquad \qquad \quad \lambda \geq 0
   $$
   Associate the constraints with Lagrange multipliers $μ$ ,  and construct the Lagrangian for this problem: 
   $$
   L(\lambda,\ μ) = λlog{\frac{1-λ}{λ}} + \mu (\lambda - \frac{1}{2})
   $$



Therefore the KKT conditions are:

1. Main Condition:
   $$
   log{\frac{1-λ}{λ}} - \frac{1}{1-λ} + \mu \geq 0
   $$



2. Dual Feasibility:
   $$
   \mu \geq 0
   $$

3. Complementary Condition:
   $$
   \mu (\lambda - \frac{1}{2}) = 0 \\
   \lambda (log{\frac{1-λ}{λ}} - \frac{1}{1-λ} + \mu) = 0
   $$

4. Primal Feasibility:

$$
\lambda - \frac{1}{2} \leq 0\\
\qquad \lambda \geq 0
$$

Transform it back to an optimal condition in p:

1. Main Condition:
   $$
   p - e^{-p} - 1 + \mu \geq 0
   $$

2. Dual Feasibility:
   $$
   \mu \geq 0
   $$

3. Complementary Condition:

$$
\mu(-\frac{1}{1+e^{-p}}+\frac{1}{2}) = 0\\
\frac{e^{−p}(p - e^{-p} - 1 + \mu)}{1+e^{−p}} = 0
$$

4. Primal Feasibility: 

$$
p \geq 0
$$

